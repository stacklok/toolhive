# vMCP Configuration with Optimizer Enabled
# This configuration enables the optimizer for semantic tool discovery

name: "vmcp-debug"

# Reference to ToolHive group containing MCP servers
groupRef: "default"

# Client authentication (anonymous for local development)
incomingAuth:
  type: anonymous

# Backend authentication (unauthenticated for local development)
outgoingAuth:
  source: inline
  default:
    type: unauthenticated

# Tool aggregation settings
aggregation:
  conflictResolution: prefix
  conflictResolutionConfig:
    prefixFormat: "{workload}_"

# Operational settings
operational:
  timeouts:
    default: 30s
  failureHandling:
    healthCheckInterval: 30s
    unhealthyThreshold: 3
    partialFailureMode: fail

# =============================================================================
# OPTIMIZER CONFIGURATION
# =============================================================================
# When enabled, vMCP exposes optim.find_tool and optim.call_tool instead of
# all backend tools directly. This reduces token usage by allowing LLMs to
# discover relevant tools on demand via semantic search.
#
# The optimizer ingests tools from all backends in the group, generates
# embeddings, and provides semantic search capabilities.

optimizer:
  # Enable the optimizer
  enabled: true
  
  # Embedding backend: "ollama", "openai-compatible", or "placeholder"
  # - "ollama": Uses local Ollama HTTP API for embeddings
  # - "openai-compatible": Uses OpenAI-compatible API (vLLM, OpenAI, etc.)
  # - "placeholder": Uses deterministic hash-based embeddings (for testing)
  embeddingBackend: placeholder
  
  # Embedding dimension (common values: 384, 768, 1536)
  # 384 is standard for all-MiniLM-L6-v2 and nomic-embed-text
  embeddingDimension: 384
  
  # Database path for storing embeddings and tool metadata
  # Requires sqlite-vec extension to be available
  dbPath: /tmp/vmcp-optimizer-debug.db
  
  # =============================================================================
  # PRODUCTION CONFIGURATIONS (Commented Examples)
  # =============================================================================
  
  # Option 1: Local Ollama (good for development/testing)
  # embeddingBackend: ollama
  # embeddingURL: http://localhost:11434
  # embeddingModel: nomic-embed-text
  # embeddingDimension: 384
  
  # Option 2: vLLM (recommended for production with GPU acceleration)
  # embeddingBackend: openai-compatible
  # embeddingURL: http://vllm-service:8000/v1
  # embeddingModel: BAAI/bge-small-en-v1.5
  # embeddingDimension: 768
  
  # Option 3: OpenAI API (cloud-based)
  # embeddingBackend: openai-compatible
  # embeddingURL: https://api.openai.com/v1
  # embeddingModel: text-embedding-3-small
  # embeddingDimension: 1536
  # (requires OPENAI_API_KEY environment variable)
  
  # Option 4: Kubernetes in-cluster service (K8s deployments)
  # embeddingService: embedding-service-name
  # (vMCP will resolve the service DNS name)

# =============================================================================
# USAGE
# =============================================================================
# 1. Start MCP backends in the group:
#    thv run weather --group default
#    thv run github --group default
#
# 2. Start vMCP with optimizer:
#    thv vmcp serve --config examples/vmcp-config-optimizer.yaml
#
# 3. Connect MCP client to vMCP
#
# 4. Available tools from vMCP:
#    - optim.find_tool: Search for tools by semantic query
#    - optim.call_tool: Execute a tool by name
#    - (backend tools are NOT directly exposed when optimizer is enabled)
